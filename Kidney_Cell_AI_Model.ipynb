{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_CW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT INFORMATION:** Two models were left on purpose so we could compare the difference between the first bad one and the second one were more funtionalities were added"
      ],
      "metadata": {
        "id": "lauRscRicVxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########IMPORTS##############"
      ],
      "metadata": {
        "id": "vxj6nkqHOJi_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "be3BoHrizWmB"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from keras.layers.core.dropout import Dropout\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model, model_from_json\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Rescaling\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from pandas import DataFrame\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########GOOGLE DRIVE MOUNT#############"
      ],
      "metadata": {
        "id": "TyfG79g4VS2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-nWEyvVNac",
        "outputId": "4c4c1600-dbcd-4242-e402-6db4a9589f59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########KAGGLE INFORMATION#############"
      ],
      "metadata": {
        "id": "SxB93rJROPdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'davidreto'  # Your Kaggle username\n",
        "os.environ['KAGGLE_KEY'] = '505caa4aee2ecd6bc8ae5f457a190351'  # Your Kaggle API key\n",
        "os.environ['URN'] = '6647000'  # Your URN: submissions without a URN will not count"
      ],
      "metadata": {
        "id": "qzemaR9QOGok"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########DATASET DOWNLOAD###############"
      ],
      "metadata": {
        "id": "mleDJvsWOYet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/gdrive/MyDrive/Colab_Notebooks/Coursework_dataset_COM2028_21-22.zip /content/Coursework_dataset_COM2028_21-22.zip\n",
        "!unzip /content/Coursework_dataset_COM2028_21-22.zip  "
      ],
      "metadata": {
        "id": "FHLWc5vmM9OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########ADDING EXTENSION AFTER THE ID TO GET THE PICTURE#################"
      ],
      "metadata": {
        "id": "M3IZkZjwPQ38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('train.csv',dtype=str)\n",
        "col = []\n",
        "for i in df[\"Id\"]:\n",
        " col.append(i+\".jpg\")\n",
        "df[\"filename\"] = col\n",
        "\n",
        "df\n",
        "  "
      ],
      "metadata": {
        "id": "qI9IfDdc2mBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################DATASET TREATING AND RETRIVAL#########################"
      ],
      "metadata": {
        "id": "n95KuMHHbhoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colour_mode = 'grayscale'\n",
        "image_size = (28, 28)\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='checkpoints',\n",
        "    save_weights_only=True,  # Record only weights (see model.save_weights vs model.save)\n",
        "    monitor='val_accuracy',  # Evaluate model using accuracy metric on the validation set\n",
        "    mode='max',  # Determine best model based on 'max' of the 'val_accuracy' score\n",
        "    save_best_only=True)  # Keep only the best-performing model (i.e. overwrite only when the model has improved)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    validation_split =0.2)\n",
        "\n",
        "train_dataset = train_datagen.flow_from_dataframe(\n",
        "  dataframe=df,\n",
        "  directory='train',\n",
        "  color_mode=colour_mode,\n",
        "  \n",
        "  y_col=\"Cell type\",\n",
        "  x_col=\"filename\",\n",
        "  subset='training',\n",
        "  seed=42,\n",
        "  class_mode='categorical',  # There are 8 classes for this task.\n",
        "  target_size=image_size, \n",
        "  batch_size=batch_size)\n",
        "\n",
        "\n",
        "validation_dataset = train_datagen.flow_from_dataframe(\n",
        "  dataframe=df,\n",
        "  directory='train',\n",
        "  color_mode=colour_mode,\n",
        "  \n",
        "  y_col=\"Cell type\",\n",
        "  x_col=\"filename\",\n",
        "  subset='validation',\n",
        "  seed=42,\n",
        "  class_mode='categorical',  # There are 8 classes for this task.\n",
        "  target_size=image_size,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "  'test',\n",
        "  shuffle=False,  # Important: the test dataset order must match Kaggle!\n",
        "  labels=None,  # Important: the test dataset has no labels...\n",
        "  color_mode=colour_mode,\n",
        "  image_size=image_size,\n",
        "  batch_size=100)  # Don't batch the test samples.\n"
      ],
      "metadata": {
        "id": "lns8Q7mqPQKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########BAD MODEL################"
      ],
      "metadata": {
        "id": "l7MWQ_z1baK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "badmodel = Sequential()\n",
        "\n",
        "badmodel.add(Rescaling(1./255)) \n",
        "\n",
        "badmodel.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
        "badmodel.add(layers.RandomRotation(0.2))\n",
        "\n",
        "badmodel.add(Conv2D(16, kernel_size=3,activation=\"relu\",input_shape=(28,28,1)))\n",
        "badmodel.add(Conv2D(16, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "\n",
        "badmodel.add(Conv2D(32, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(Conv2D(32, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "badmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "badmodel.add(Dropout(0.2))\n",
        "\n",
        "badmodel.add(Conv2D(64, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(Conv2D(64, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "\n",
        "badmodel.add(Dropout(0.2))\n",
        "\n",
        "badmodel.add(Conv2D(128, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(Conv2D(128, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "badmodel.add(Conv2D(256, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(Conv2D(256, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "badmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "badmodel.add(Flatten())\n",
        "badmodel.add(Dropout(0.5))\n",
        "badmodel.add(Dense(256,activation=\"relu\"))\n",
        "badmodel.add(Dense(512,activation=\"relu\"))\n",
        "badmodel.add(Dense(8,activation=\"softmax\"))\n",
        "\n",
        "badmodel.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "\n",
        "# Now train the model...\n",
        "\n",
        "# Set up checkpointing callback, called when training:\n",
        "\n",
        "\n",
        "history = badmodel.fit(train_dataset, validation_data=validation_dataset, epochs=30,callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "Ss6pEhrKz3Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########## ACURACY AND LOSS PLOTING FOR BAD MODEL################"
      ],
      "metadata": {
        "id": "jVf9db5TbQEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "loss_train = history.history['accuracy']\n",
        "loss_val = history.history['val_accuracy']\n",
        "epochs = range(1,41)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training Accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,41)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lAeyN7wmSnMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####################FINAL MODEL######################"
      ],
      "metadata": {
        "id": "A56nfosWbIJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Rescaling(1./255)) \n",
        "\n",
        "model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
        "model.add(layers.RandomRotation(0.2))\n",
        "\n",
        "model.add(Conv2D(16, kernel_size=3,activation=\"relu\",input_shape=(28,28,1)))\n",
        "model.add(Conv2D(16, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(Conv2D(32, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(Conv2D(64, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(Conv2D(128, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(Conv2D(256, kernel_size=3,activation=\"relu\", padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256,activation=\"relu\"))\n",
        "model.add(Dense(512,activation=\"relu\"))\n",
        "model.add(Dense(8,activation=\"softmax\"))\n",
        "lr_reduce=keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.2,\n",
        "    patience=4,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    min_lr=0.00001,\n",
        ")\n",
        "model.compile(optimizer='adamax',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "\n",
        "# Now train the model...\n",
        "\n",
        "# Set up checkpointing callback, called when training:\n",
        "\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=40,callbacks=[checkpoint_callback,lr_reduce])"
      ],
      "metadata": {
        "id": "6_dBJgPynsDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########ACURACY AND LOSS PLOTING FOR FINAL MODEL################"
      ],
      "metadata": {
        "id": "n_zJrW90a1eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "loss_train = history.history['accuracy']\n",
        "loss_val = history.history['val_accuracy']\n",
        "epochs = range(1,41)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training Accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,41)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vobQIuaLjp-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##################################PREDICTIONS###########################\n"
      ],
      "metadata": {
        "id": "x07iHvp3aZOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kidney_array = []\n",
        "i=0\n",
        "# Note:if you want to test the bad model you would need to change from model.predict to badmodel.predict\n",
        "predictions = model.predict(test_dataset)\n",
        "\n",
        "while i <  len(predictions):\n",
        "  if np.argmax(predictions[i])==0 :\n",
        "    kidney_array.append(\"0\")\n",
        "\n",
        "  elif np.argmax(predictions[i])==1 :\n",
        "    kidney_array.append(\"1\")    \n",
        "\n",
        "  elif np.argmax(predictions[i])==2 :\n",
        "    kidney_array.append(\"2\")\n",
        "\n",
        "  elif np.argmax(predictions[i])==3 :\n",
        "    kidney_array.append(\"3\")\n",
        "\n",
        "  elif np.argmax(predictions[i])==4 :\n",
        "    kidney_array.append(\"4\")\n",
        "\n",
        "  elif np.argmax(predictions[i])==5 :\n",
        "    kidney_array.append(\"5\")\n",
        "\n",
        "  elif np.argmax(predictions[i])==6 :\n",
        "    kidney_array.append(\"6\")\n",
        "\n",
        "  elif np.argmax(predictions[i])==7 :\n",
        "    kidney_array.append(\"7\")\n",
        "\n",
        "  i=i+1\n",
        "DataFrame(data={'Cell type': kidney_array}).to_csv('predictions.csv', index_label='Id')"
      ],
      "metadata": {
        "id": "PLNycaZEmxMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################KAGLE SUBMISSION#####################"
      ],
      "metadata": {
        "id": "19gGpBQmaLR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /usr/local/bin/kaggle competitions submit -m $URN -c uos-com2028-21-22-cw -f predictions.csv"
      ],
      "metadata": {
        "id": "IgQU9mmHxp-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}